{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import cv2\n",
    "from tracker import Tracker \n",
    "from utils import select_bounding_box\n",
    "from video_read_thread import FileVideoStream\n",
    "import time\n",
    "import imutils\n",
    "from random import randint\n",
    "from imutils.video import FPS\n",
    "from people_recognize import PersonRecognizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\PYT\\Repos_own\\Video Retail Analysis\\model\\keras_layers\\keras_layer_DecodeDetections.py:174: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\PYT\\Repos_own\\Video Retail Analysis\\model\\keras_loss_function\\keras_ssd_loss.py:166: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'\n",
    "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
    "person_detector = PersonRecognizer()\n",
    "\n",
    "def get_faces_from_haar(frame):\n",
    "    bboxes=[]\n",
    "    colors=[]\n",
    "    frame=imutils.resize(frame,width=500)\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    colors.append((randint(0, 255), randint(0, 255), randint(0, 255)))\n",
    "    faces=face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(5,5),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    print(len(faces),\" faces found in the frame\")\n",
    "    if len(faces)>0:\n",
    "        for face in faces:\n",
    "            (x,y,w,h) = faces[0]\n",
    "            bboxes.append((x,y,w,h))\n",
    "            frame2=frame.copy()\n",
    "            frame2 = cv2.rectangle(frame2,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            while cv2.waitKey(1) & 0xFF != ord('q'):\n",
    "                cv2.imshow(\"Press q to continue\",frame2)\n",
    "    return bboxes,colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people_from_frame(frame,recognizer=person_detector):\n",
    "    height,width,channels=frame.shape\n",
    "    boxes = recognizer.detect_people(frame)\n",
    "    bboxes =[]\n",
    "    colors = []\n",
    "    for box in boxes :\n",
    "        bboxes.append((int(box[-4]*width/512),int(box[-3]*height/512),int(box[-2]*width/512),int(box[-1])*height/512))\n",
    "        colors.append((randint(0, 255), randint(0, 255), randint(0, 255)))\n",
    "    return bboxes,colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set video to load\n",
    "#videoPath = r\"video/VID20200315124213.mp4\"\n",
    "videoPath = r\"C:\\Users\\HP\\PYT\\Repos_dowloaded\\people-counting-opencv\\videos/example_01.mp4\"\n",
    "# Create a video capture object to read videos\n",
    "cap=FileVideoStream(videoPath).start() \n",
    "# Read first frame\n",
    "success, frame = cap.read()\n",
    "# quit if unable to read the video file\n",
    "#time.sleep(1.0)\n",
    "#if not success:\n",
    "#    print('Failed to read video')\n",
    "#    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "Tracker Created ...\n",
      "[INFO] elasped time: 97.95\n",
      "[INFO] approx. FPS: 3.97\n"
     ]
    }
   ],
   "source": [
    "#print(bboxes)\n",
    "#bboxes,colors=select_bounding_box(frame)\n",
    "#bboxes,colors=get_faces_from_haar(frame)\n",
    "\n",
    "fps = FPS().start()\n",
    "fr = 30\n",
    "\n",
    "# Process video and track objects\n",
    "while cap.more():\n",
    "    success, frame = cap.read()\n",
    "    fr -=1 \n",
    "    if fr == 0 :\n",
    "        fr = 30\n",
    "    if not success:\n",
    "        break\n",
    "    if fr == 30 :\n",
    "        bboxes,colors=detect_people_from_frame(frame)\n",
    "        multiTracker = Tracker(bboxes,frame).multiTracker\n",
    "# get updated location of objects in subsequent frames\n",
    "    success, boxes = multiTracker.update(frame)\n",
    "# draw tracked objects\n",
    "    for i, newbox in enumerate(boxes):\n",
    "        p1 = (int(newbox[0]), int(newbox[1]))\n",
    "        #p2 = (int(newbox[0] + newbox[2]), int(newbox[1] + newbox[3]))\n",
    "        p2 = (int( newbox[2]), int( newbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, colors[i], 2, 1)\n",
    "# show frame\n",
    "    cv2.putText(frame,str(cap.qsize()),(30,30),cv2.FONT_HERSHEY_COMPLEX,1,(255,0,0),2)\n",
    "    cv2.imshow('MultiTracker', frame)\n",
    "# quit on 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  \n",
    "        break\n",
    "    fps.update()\n",
    "cap.stop()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "fps.stop()\n",
    "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
